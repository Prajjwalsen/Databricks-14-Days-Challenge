# ğŸ“… Day 04 â€“ Delta Lake Introduction

This day focused on understanding **Delta Lake fundamentals** and implementing core features like **ACID transactions, schema enforcement, and duplicate handling** using both **PySpark and SQL** in Databricks.

---

## ğŸ“˜ What I Learned

- What is **Delta Lake** and why it is used
- Difference between **Delta vs Parquet**
- Importance of **ACID transactions**
- **Schema enforcement** in Delta tables
- Handling **duplicate records** using MERGE operations

---

## ğŸ› ï¸ Tasks Completed

### 1ï¸âƒ£ Convert CSV to Delta Format
Converted the input dataset into **Delta format** using PySpark.

ğŸ“¸ **Screenshot:**  
![Convert to Delta](Screenshots/convert_to_delta.png)

---

### 2ï¸âƒ£ Create Delta Tables (PySpark & SQL)

Created a **managed Delta table** using PySpark and verified it using SQL queries.

ğŸ“¸ **Create Delta Table using PySpark:**  
![Create Delta Table PySpark](Screenshots/create_delta_table_pyspark.png)

ğŸ“¸ **Verify Delta Table using SQL:**  
![Create Delta Table SQL](Screenshots/create_delta_table_sql.png)

---

### 3ï¸âƒ£ Schema Enforcement Test

Tested Delta Lakeâ€™s schema enforcement by trying to append data with an incorrect schema.  
Delta Lake correctly **blocked the write operation**.

ğŸ“¸ **Schema Enforcement Error:**  
![Schema Enforcement Error](Screenshots/schema_enforcement_error.png)

---

### 4ï¸âƒ£ Handle Duplicate Inserts

Handled duplicate records using **deduplication logic** and **MERGE operation**.

#### ğŸ” Identify Duplicate Records
![Duplicate Insert](Screenshots/duplicate_insert.png)

#### ğŸ§¹ Create Deduplicated Source View
![Deduplicated Source View](Screenshots/deduplicated_source_view.png)

#### ğŸ” Merge Deduplicated Data into Delta Table
![Merge After Deduplication](Screenshots/merge_after_deduplication.png)

#### âœ… Final Table Without Duplicates
![Final No Duplicates](Screenshots/final_no_duplicates.png)

---

## âœ… Key Takeaways

- Delta Lake provides **ACID guarantees** for big data workloads
- **Schema enforcement** prevents bad data ingestion
- **MERGE + deduplication** is the correct way to handle duplicates
- Delta Lake is reliable, scalable, and production-ready

---

## ğŸ“Œ Summary

Day 04 helped me understand how Delta Lake improves data reliability and consistency in real-world data engineering pipelines.  
All tasks were successfully implemented and validated using Databricks.

---

ğŸ“‚ **Folder Structure:**

```text
Day 04/
â”œâ”€â”€ README.md
â””â”€â”€ Screenshots/
    â”œâ”€â”€ convert_to_delta.png
    â”œâ”€â”€ create_delta_table_pyspark.png
    â”œâ”€â”€ create_delta_table_sql.png
    â”œâ”€â”€ schema_enforcement_error.png
    â”œâ”€â”€ duplicate_insert.png
    â”œâ”€â”€ deduplicated_source_view.png
    â”œâ”€â”€ merge_after_deduplication.png
    â””â”€â”€ final_no_duplicates.png

