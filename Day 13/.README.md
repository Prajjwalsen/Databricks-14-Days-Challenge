# ğŸ¯ Day 13 â€“ Model Comparison & Feature Engineering (Databricks)

## ğŸš€ Databricks 14 Days AI Challenge  
This document captures my learning and hands-on implementation for **Day 13** of the  
**Databricks 14 Days AI Challenge** by **Indian Data Club**.

---

## ğŸ“Œ Topics Covered
- Training multiple ML models
- Model comparison using MLflow
- Feature engineering concepts
- Hyperparameter awareness
- Spark ML Pipelines
- Best model selection strategy

---

## ğŸ› ï¸ Tasks Completed

âœ… Trained 3 different regression models  
âœ… Logged and compared models using MLflow  
âœ… Analyzed RÂ² scores across models  
âœ… Built Spark ML Pipeline  
âš ï¸ Best model selection faced expected limitations due to small dataset  

---

## ğŸ§ª Hands-on Implementation

### ğŸ”¹ 1. Data Preparation
Prepared clean and structured dataset for training multiple models.

![Data Preparation](Screenshots/data_preparation.png)

---

### ğŸ”¹ 2. MLflow Model Comparison
Trained and logged multiple models using MLflow:
- Linear Regression
- Decision Tree Regressor
- Random Forest Regressor

Each model run was tracked separately for comparison.

![MLflow Model Comparison](Screenshots/mlflow_model_comparison%20(1).png)

---

### ğŸ”¹ 3. Model Evaluation (RÂ² Scores)
Evaluated all trained models using RÂ² score to compare their performance.

![Model R2 Scores](Screenshots/model_r2_scores.png)

âš ï¸ **Note:**  
RÂ² scores appeared as `NaN` due to extremely small dataset size, which is expected behavior when test data contains insufficient samples.

---

### ğŸ”¹ 4. Spark ML Pipeline
Built an end-to-end Spark ML Pipeline using:
- `VectorAssembler`
- `LinearRegression`
- `Pipeline` API

![Spark ML Pipeline](Screenshots/spark_ml_pipeline.png)

---

### ğŸ”¹ 5. Best Model Selection (Limitation Observed)
Attempted to select the best model based on evaluation metrics.

![Best Model Selection Error](Screenshots/best_model_selection_error.png)

âš ï¸ **Reason:**  
Due to very limited data in Databricks Community Edition, Spark ML evaluation summarizer failed.  
This is an expected limitation when working with extremely small datasets.

---

### ğŸ”¹ 6. MLflow Runs Verification
Verified successful logging of multiple model runs in MLflow experiment tracking.

![MLflow Runs](Screenshots/Screenshot%202026-01-21%20182648.png)

---

## ğŸ§  Key Takeaways
- MLflow makes model comparison simple and reproducible
- Training multiple models helps understand performance trade-offs
- Feature engineering plays a critical role in model quality
- Spark ML Pipelines standardize ML workflows
- Small datasets can limit metric evaluation (expected in Community Edition)

---

## ğŸ“‚ Repository Structure
```text
Day-13/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ Screenshots/
    â”œâ”€â”€ data_preparation.png
    â”œâ”€â”€ mlflow_model_comparison (1).png
    â”œâ”€â”€ model_r2_scores.png
    â”œâ”€â”€ spark_ml_pipeline.png
    â”œâ”€â”€ best_model_selection_error.png
    â””â”€â”€ Screenshot 2026-01-21 182648.png

