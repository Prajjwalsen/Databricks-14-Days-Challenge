# Databricks-14-Days-Challenge
This repository contains my daily progress, notebooks, and learnings from the Databricks 14 Days AI Challenge by Indian Data Club.

## ğŸ“… Day 01 â€“ Platform Setup & First Steps (Databricks)

### ğŸ“˜ What I Learned
- Why Databricks is preferred over Pandas / Hadoop
- Basics of Lakehouse Architecture
- Databricks Workspace structure
- Introduction to PySpark DataFrames
- Loading external datasets using Kaggle API

### âœ… Tasks Completed
- Created Databricks Community Edition account
- Explored Workspace, Compute, and Catalog
- Created first Databricks notebook
- Ran basic PySpark commands
- Downloaded and loaded Kaggle dataset into Databricks

ğŸ“‚ **Detailed implementation with screenshots:**  
â¡ï¸ [View Day-01 Documentation](Day-01/)

---


## ğŸ“… Day 02 â€“ PySpark DataFrame Operations

### ğŸ“˜ What I Learned
- Creating PySpark DataFrames using `Row`
- Understanding DataFrame schema with `printSchema()`
- Selecting specific columns from a DataFrame
- Filtering records using conditions
- Aggregating data using `groupBy`
- Sorting data using `orderBy`

### âœ… Tasks Completed
- Created sample E-commerce events DataFrame
- Explored DataFrame schema
- Performed select and filter operations
- Applied groupBy and orderBy for analysis

ğŸ“‚ **Detailed implementation with screenshots:**  
â¡ï¸ [View Day-02 Documentation](Day-02/)

---
## ğŸ“… Day 03 â€“ PySpark Transformations Deep Dive

### ğŸ“˜ What I Learned
- PySpark vs Pandas comparison
- Performing joins (inner, left, right, outer)
- Revenue calculation using aggregations
- Window functions (running totals & ranking)
- Conversion rate analysis
- Creating and using User-Defined Functions (UDFs)

### âœ… Tasks Completed
- Loaded full e-commerce dataset
- Performed complex joins
- Calculated top products by revenue
- Implemented window functions
- Derived conversion metrics
- Built custom UDFs

ğŸ“‚ **Detailed implementation with screenshots:**  
â¡ï¸ [View Day-03 Documentation](Day-03/)

---

## ğŸ“… Day 04 â€“ Delta Lake Introduction

### ğŸ“˜ What I Learned
- What is Delta Lake and why it is used
- ACID transactions in big data
- Schema enforcement & validation
- Delta vs Parquet
- Handling duplicate data using MERGE

### âœ… Tasks Completed
- Converted CSV data into Delta format
- Created Delta tables using PySpark and SQL
- Tested schema enforcement with invalid schema
- Detected and handled duplicate records using deduplication + MERGE

ğŸ“‚ **Detailed implementation with screenshots:**  
â¡ï¸ [View Day-04 Documentation](Day-04/)

---

## ğŸ“‚ Repository Structure

```text
Databricks-14-Days-Challenge/
â”‚
â”œâ”€â”€ Day-01/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ Screenshots/
â”‚
â”œâ”€â”€ Day 02/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ Screenshots/
â”‚
â”œâ”€â”€ Day 03/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ Screenshots/
â”‚
â”œâ”€â”€ Day 04/
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ Screenshots/
â”‚
â””â”€â”€ README.md   # Main project overview

